{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "492a7bac",
   "metadata": {},
   "source": [
    "# Calculate PPL\n",
    "\n",
    "In this notebook we shows how we retrieve the cross entropy and perplexity, use perplexity to determine the authorship of question documents, and log the benchmark result against true labels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10da2a24",
   "metadata": {},
   "source": [
    "Install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15add47f-f462-4d23-8910-cdfd7e208be2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T05:06:04.146840Z",
     "iopub.status.busy": "2023-09-15T05:06:04.146501Z",
     "iopub.status.idle": "2023-09-15T05:06:47.889563Z",
     "shell.execute_reply": "2023-09-15T05:06:47.888597Z",
     "shell.execute_reply.started": "2023-09-15T05:06:04.146813Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U attrs setuptools wheel\n",
    "!pip install cupy-cuda11x\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_trf\n",
    "!pip3 install tqdm torch transformers datasets torchvision torchaudio "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "041cc659",
   "metadata": {},
   "source": [
    "Load packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13340cba-e764-4abb-a0c9-0b503c8edfd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T05:06:51.937881Z",
     "iopub.status.busy": "2023-09-15T05:06:51.937205Z",
     "iopub.status.idle": "2023-09-15T05:06:52.342009Z",
     "shell.execute_reply": "2023-09-15T05:06:52.341088Z",
     "shell.execute_reply.started": "2023-09-15T05:06:51.937852Z"
    }
   },
   "outputs": [],
   "source": [
    "import os,shutil,glob,math,ast,copy,io,zipfile,csv\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2LMHeadModel,GPT2TokenizerFast,GPT2Tokenizer,AutoTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel \n",
    "pandarallel.initialize(nb_workers=6)\n",
    "\n",
    "import spacy\n",
    "spacy.require_gpu()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from multiprocess import Pool\n",
    "proc_num=8\n",
    "\n",
    "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d72e871",
   "metadata": {},
   "source": [
    "Set up variables here.\n",
    "\n",
    "test_data_path: the path to the test.csv for testing.\n",
    "\n",
    "model_path: the path containing the fine-tuned authorial GPT-2s.\n",
    "\n",
    "ce_log_home: the directory to hold logs for cross entropy\n",
    "\n",
    "device: the device to boost inferring. \"cuda\" by default.\n",
    "\n",
    "key_var_tag: the id label. For GEFA, use author_tag.\n",
    "\n",
    "ppl_dfs_buffer_home: the directory to hold ppl calculation results.\n",
    "\n",
    "test_text_limits: test text lengths used in evaluating.\n",
    "\n",
    "pred_df_buffer_home: the directory to hold text by text test results against true labels. \n",
    "\n",
    "benchmark_results_df_home: the directory to hold evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2b6093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T05:06:52.344630Z",
     "iopub.status.busy": "2023-09-15T05:06:52.344116Z",
     "iopub.status.idle": "2023-09-15T05:06:52.349575Z",
     "shell.execute_reply": "2023-09-15T05:06:52.348776Z",
     "shell.execute_reply.started": "2023-09-15T05:06:52.344604Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data_path=\"corpus/GEFA-full/test.csv\"\n",
    "model_path=\"model\"\n",
    "ce_log_home=\"ce_log\"\n",
    "result_path=\"ppl_result.csv\"\n",
    "device=\"cuda\"\n",
    "key_var_tag=\"author_tag\"\n",
    "ppl_dfs_buffer_home=\"ppl_dfs_buffer\"\n",
    "test_text_limits=[100,200,300,400,50,150,250,350,450,10,20,30,40,60,70,80,90]\n",
    "pred_df_buffer_home=\"pred_df_buffer\"\n",
    "benchmark_results_df_home=\"benchmark_results_df_home\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07ae098f",
   "metadata": {},
   "source": [
    "Load working environments & datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6940ba7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T05:06:52.351037Z",
     "iopub.status.busy": "2023-09-15T05:06:52.350791Z",
     "iopub.status.idle": "2023-09-15T05:06:54.249463Z",
     "shell.execute_reply": "2023-09-15T05:06:54.248680Z",
     "shell.execute_reply.started": "2023-09-15T05:06:52.351006Z"
    }
   },
   "outputs": [],
   "source": [
    "model_author_tags=os.listdir(model_path)\n",
    "model_author_tags.sort()\n",
    "model_author_tags=model_author_tags\n",
    "\n",
    "total_dataset=Dataset.from_csv(test_data_path)\n",
    "text_author_tags=list(set(total_dataset[key_var_tag]))\n",
    "\n",
    "if(not(os.path.isfile(result_path))):\n",
    "    with open(result_path,\"w\",-1,\"utf-8\") as f:\n",
    "        f.write(\"\")\n",
    "      \n",
    "if(not(os.path.isdir(ce_log_home))):os.mkdir(ce_log_home)\n",
    "\n",
    "test_sets={}\n",
    "for text_author_tag in text_author_tags:\n",
    "    test_sets[str(text_author_tag)]=total_dataset.filter(lambda daton:str(daton[key_var_tag])==str(text_author_tag))\n",
    "\n",
    "if(not(os.path.isdir(ppl_dfs_buffer_home))):os.mkdir(ppl_dfs_buffer_home)\n",
    "if(not(os.path.isdir(pred_df_buffer_home))):os.mkdir(pred_df_buffer_home)\n",
    "if(not(os.path.isdir(benchmark_results_df_home))):os.mkdir(benchmark_results_df_home)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7124a80b",
   "metadata": {},
   "source": [
    "Initiate spacy for on-the-fly tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6b800f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T05:06:54.250784Z",
     "iopub.status.busy": "2023-09-15T05:06:54.250527Z",
     "iopub.status.idle": "2023-09-15T05:07:02.832473Z",
     "shell.execute_reply": "2023-09-15T05:07:02.831553Z",
     "shell.execute_reply.started": "2023-09-15T05:06:54.250754Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy.require_gpu()\n",
    "nlp=spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5250688d",
   "metadata": {},
   "source": [
    "Start calculating cross entropy and on the fly tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9faa9-031c-44dd-b66e-fdc05d7a74d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T05:07:29.065046Z",
     "iopub.status.busy": "2023-09-15T05:07:29.064804Z"
    }
   },
   "outputs": [],
   "source": [
    "for model_author_tag in model_author_tags:\n",
    "\n",
    "    model_id=os.path.join(model_path,model_author_tag)\n",
    "\n",
    "    model=GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "    tokenizer=AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    for text_author_tag in text_author_tags:\n",
    "        \n",
    "        # \n",
    "        logged_tag_pairs=[]\n",
    "        with open(result_path,\"r\",-1,\"utf-8\") as f:\n",
    "            csvr=csv.reader(f)\n",
    "            for r in csvr:\n",
    "                logged_tag_pairs.append((r[0],r[1]))\n",
    "        selected_pairs=list(filter(lambda unit:(str(unit[0])==str(model_author_tag))and(str(unit[1])==str(text_author_tag)),logged_tag_pairs))\n",
    "        if(selected_pairs.__len__()>0):\n",
    "            print(f\"skipped logged pair {str(model_author_tag)},{str(text_author_tag)}\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"start processing pair {str(model_author_tag)},{str(text_author_tag)}\")       \n",
    "        \n",
    "        # set up test set\n",
    "        \n",
    "        test_set=test_sets[str(text_author_tag)]\n",
    "        records=[]\n",
    "        nlls=[]\n",
    "        for test_text_sample in tqdm(test_set[\"text\"]):\n",
    "            test_text=test_text_sample.replace(\"<BOS>\",\"\").replace(\"<EOS>\",\"\")\n",
    "            encodings=tokenizer(test_text,return_tensors=\"pt\")# tokenize\n",
    "            \n",
    "            # On the fly pos-tagging & dep & ner, using the tokens from gpt2 tokenizer\n",
    "            \n",
    "            words=tokenizer.batch_decode(encodings.input_ids[0][:]) # decode text for spacy tagger\n",
    "            tokens=list(words)\n",
    "            lemmas=[]\n",
    "            poss=[]\n",
    "            tags=[]\n",
    "            shapes=[]\n",
    "            alphas=[]\n",
    "            stops=[]\n",
    "            morphs=[]\n",
    "            deps=[]\n",
    "            ent_types=[]\n",
    "            raw_doc=spacy.tokens.Doc(nlp.vocab,words)\n",
    "            doc=nlp(raw_doc)\n",
    "            for token in doc:\n",
    "                lemmas.append(token.lemma_)\n",
    "                poss.append(token.pos_)\n",
    "                tags.append(token.tag_)\n",
    "                shapes.append(token.shape_)\n",
    "                alphas.append(token.is_alpha)\n",
    "                stops.append(token.is_stop)\n",
    "                morphs.append(str(token.morph))\n",
    "                deps.append(token.dep_)\n",
    "                ent_types.append(token.ent_type_)\n",
    "\n",
    "            # Calculate and record cross entropy loss\n",
    "            losses=[]\n",
    "            max_length=model.config.n_positions\n",
    "            stride=128\n",
    "            seq_len=encodings.input_ids.size(1)\n",
    "            prev_end_loc = 0\n",
    "            for begin_loc in range(0, seq_len, stride):\n",
    "                end_loc = min(begin_loc + max_length, seq_len)\n",
    "                trg_len = end_loc - prev_end_loc\n",
    "                input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "                target_ids=input_ids.clone()\n",
    "                target_ids[:, :-trg_len]=-100\n",
    "                with torch.no_grad():\n",
    "                    outputs=model(input_ids, labels=target_ids)\n",
    "                    pooled_logits=outputs.logits\n",
    "                    shift_labels=input_ids[..., 1:].contiguous()\n",
    "                    shift_logits=pooled_logits[..., :-1, :].contiguous()\n",
    "                    for i in range(0,shift_logits.size()[1]):\n",
    "                        target_labels=shift_labels[...,i].contiguous()\n",
    "                        target_logits=shift_logits[...,i,:].contiguous()\n",
    "                        loss_fct=CrossEntropyLoss()\n",
    "                        loss=loss_fct(target_logits.view(-1, target_logits.size(-1)), target_labels.view(-1))\n",
    "                        losses.append(float(loss))\n",
    "                neg_log_likelihood=outputs.loss\n",
    "                nlls.append(neg_log_likelihood)\n",
    "                prev_end_loc = end_loc\n",
    "                if end_loc == seq_len:\n",
    "                    break\n",
    "            record=[tokens,losses,lemmas,poss,tags,shapes,alphas,stops,morphs,deps,ent_types]\n",
    "            records.append(record)\n",
    "        \n",
    "        # Write text_by_text cross entropy data\n",
    "        \n",
    "        ce_log_fn=\"-\".join([str(model_author_tag),str(text_author_tag)])+\".csv\"\n",
    "        ce_log_fn_zip=ce_log_fn+\".7z\"\n",
    "        with zipfile.ZipFile(os.path.join(ce_log_home,ce_log_fn_zip),\"w\",compression=zipfile.ZIP_LZMA) as z:\n",
    "            with z.open(ce_log_fn,\"w\") as f:\n",
    "                text_f=io.TextIOWrapper(f,\"utf-8\")\n",
    "                csvw=csv.writer(text_f)\n",
    "                csvw.writerows(records)\n",
    "                text_f.flush()\n",
    "        \n",
    "        # Write result log to the csv at result_path\n",
    "\n",
    "        ppl=torch.exp(torch.stack(nlls).mean())# caluclate per author perplexity -- this is for earlier stage research\n",
    "        result_record=[str(model_author_tag),str(text_author_tag),str(float(stride)),str(float(ppl))]\n",
    "        record_line=\",\".join(result_record)\n",
    "        print(record_line)\n",
    "\n",
    "        with open(result_path,\"a\",-1,\"utf-8\") as f:\n",
    "            f.write(record_line+\"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "289b29ba",
   "metadata": {},
   "source": [
    "Start calculating perplexity per text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be5702",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_base=os.path.join(ppl_dfs_buffer_home,\"ppl_dfs_buffer-full-[TEST_TEXT_LIMITS].csv\")\n",
    "\n",
    "template_feature_categories=[\"tokens\",\"losses\",\"lemmas\",\"poss\",\"tags\",\"shapes\",\"alphas\",\"stops\",\"morphs\",\"deps\",\"ent_types\"]\n",
    "selected_all=['losses_shifted','losses']\n",
    "\n",
    "def resstr2list(item):\n",
    "    uneq_counts=[]\n",
    "    for feature_category in feature_categories:\n",
    "        if(type(test_text_limit)==type(0)):\n",
    "            item[feature_category]=ast.literal_eval(str(item[feature_category]))[0:test_text_limit]\n",
    "            item[feature_category+\"_count\"]=item[feature_category].__len__()\n",
    "        else:\n",
    "            item[feature_category]=ast.literal_eval(str(item[feature_category]))\n",
    "            item[feature_category+\"_count\"]=item[feature_category].__len__()\n",
    "        uneq_counts.append(item[feature_category+\"_count\"])\n",
    "    if(not(item[\"losses_count\"]==(item[\"tokens_count\"]-1))):\n",
    "        item[\"losses\"]=item[\"losses\"][0:(item[\"tokens_count\"]-1)]\n",
    "    item[\"losses\"]=item[\"losses\"]+[(float(0))]\n",
    "    item[\"losses_count\"]=item[\"losses\"].__len__()\n",
    "    uneq_counts.append(item[\"losses_count\"])\n",
    "    item[\"losses_shifted\"]=[(float(0))]+item[\"losses\"][:-1]\n",
    "    item[\"losses_shifted_count\"]=item[\"losses_shifted\"].__len__()\n",
    "    uneq_counts.append(item[\"losses_shifted_count\"])\n",
    "    item[\"uneq_alarm\"]=(list(set(uneq_counts)).__len__()==1)\n",
    "    return item\n",
    "\n",
    "def corpusrow2textdf(item):\n",
    "    item[\"text_df\"]=pd.DataFrame([item[feature_categories]]).explode(feature_categories)\n",
    "    return item\n",
    "\n",
    "def cal_ppl(losses):\n",
    "    if(losses.__len__()>0):\n",
    "        ppl=math.exp(sum(losses)/losses.__len__())\n",
    "    else:\n",
    "        ppl=0\n",
    "    return ppl\n",
    "\n",
    "def cal_ppl_global(item):\n",
    "    for aim_category in selected_all:\n",
    "        losses=list(item[\"text_df\"][aim_category])\n",
    "        ppl=cal_ppl(losses)\n",
    "        ppl_info=f\"global-ppl:({aim_category})\"\n",
    "        item[ppl_info]=ppl\n",
    "    return item\n",
    "\n",
    "fps=glob.glob(os.path.join(ce_log_home,\"*.csv.7z\"),recursive=True)\n",
    "model_tags=[]\n",
    "text_tags=[]\n",
    "for fp in fps:\n",
    "    bn=os.path.basename(fp).split(\".csv.7z\")[0]\n",
    "    tags=bn.split(\"-\")\n",
    "    model_tags.append(tags[0])\n",
    "    text_tags.append(tags[1])\n",
    "model_tags=list(set(model_tags))\n",
    "text_tags=list(set(text_tags))\n",
    "\n",
    "tasks=[(model_tag,text_tag) for model_tag in model_tags for text_tag in text_tags]\n",
    "\n",
    "for test_text_limit in test_text_limits:\n",
    "\n",
    "    output_path=output_path_base.replace(\"[TEST_TEXT_LIMITS]\",str(test_text_limit))\n",
    "\n",
    "    ppl_dfs=pd.DataFrame()\n",
    "\n",
    "    for task in tqdm(tasks):\n",
    "\n",
    "        feature_categories=copy.deepcopy(template_feature_categories)\n",
    "\n",
    "        fp=os.path.join(ce_log_home,f\"{task[0]}-{task[1]}.csv.7z\")\n",
    "\n",
    "        corpus_df=pd.read_csv(fp,names=feature_categories,compression=\"zip\")\n",
    "\n",
    "        corpus_df=corpus_df.parallel_apply(resstr2list,axis=1)\n",
    "\n",
    "        feature_categories=feature_categories+[\"losses_shifted\"]\n",
    "\n",
    "        text_dfs=pd.DataFrame([corpus_df.parallel_apply(corpusrow2textdf,axis=1)[\"text_df\"]]).transpose()\n",
    "\n",
    "        ppl_df=ppl_df.parallel_apply(cal_ppl_global,axis=1)\n",
    "        ppl_df=ppl_df.filter(regex=\"ppl:\",axis=1)\n",
    "\n",
    "        ppl_df[\"candidate_tag\"]=task[0]\n",
    "        ppl_df[\"true_tag\"]=task[1]\n",
    "\n",
    "        ppl_df=ppl_df.reset_index().rename(columns={\"index\":\"text_num\"}).set_index([\"true_tag\",\"candidate_tag\",\"text_num\"])\n",
    "\n",
    "        ppl_dfs=pd.concat([ppl_dfs,ppl_df])\n",
    "\n",
    "    ppl_dfs.to_csv(output_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cf35ca4",
   "metadata": {},
   "source": [
    "Use perplexity per text to determine author, benchmark result against true labels, and then generate evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fa7bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_dfs_buffer_template=os.path.join(ppl_dfs_buffer_home,\".csv\")\n",
    "pred_df_buffer_template=os.path.join(pred_df_buffer_home,\"pred_df_buffer-[TRAIN_TEXT_LIMIT]-[TEST_TEXT_LIMIT].csv\")\n",
    "\n",
    "def ppl_dfs_2_pred_df(ppl_dfs_buffer_paths):\n",
    "\n",
    "    for ppl_dfs_buffer_path in tqdm(ppl_dfs_buffer_paths):\n",
    "\n",
    "        train_text_limit=os.path.basename(ppl_dfs_buffer_path).split(\".csv\")[0].split(\"-\")[1]\n",
    "        test_text_limit=os.path.basename(ppl_dfs_buffer_path).split(\".csv\")[0].split(\"-\")[2]\n",
    "        pred_df_buffer_path=pred_df_buffer_template.replace(\"[TEST_TEXT_LIMIT]\",test_text_limit).replace(\"[TRAIN_TEXT_LIMIT]\",train_text_limit)\n",
    "        # print(pred_df_buffer_path)\n",
    "\n",
    "        ppl_dfs=pd.read_csv(ppl_dfs_buffer_path)\n",
    "        true_tags=list(set(ppl_dfs[\"true_tag\"]))\n",
    "        candidate_tags=list(set(ppl_dfs[\"candidate_tag\"]))\n",
    "        ppl_dfs=ppl_dfs.set_index(\"candidate_tag\")\n",
    "\n",
    "        by_features=[column for column in ppl_dfs.columns if \"ppl:\" in column]\n",
    "\n",
    "        pred_df=pd.DataFrame()\n",
    "\n",
    "        for by_feature in by_features: \n",
    "\n",
    "            for true_tag in true_tags:\n",
    "\n",
    "                ppl_dfs_batch=ppl_dfs[ppl_dfs[\"true_tag\"]==true_tag]\n",
    "\n",
    "                text_nums=list(set(ppl_dfs_batch[\"text_num\"]))\n",
    "\n",
    "                pred_logs=[]\n",
    "\n",
    "                for text_num in text_nums:\n",
    "                    \n",
    "                    text_res=ppl_dfs_batch[ppl_dfs_batch[\"text_num\"]==text_num]\n",
    "                    feat_vec=text_res[by_feature]\n",
    "                    pred_tag=feat_vec.idxmin()\n",
    "\n",
    "                    pred_log={\"by\":by_feature,\"true_tag\":true_tag,\"text_num\":text_num,\"pred_tag\":pred_tag}\n",
    "                    pred_logs.append(pred_log)\n",
    "\n",
    "                pred_logs=pd.DataFrame(pred_logs)\n",
    "                pred_df=pd.concat([pred_df,pred_logs])\n",
    "\n",
    "        pred_df.to_csv(pred_df_buffer_path)\n",
    "\n",
    "ppl_dfs_buffer_paths=glob.glob(ppl_dfs_buffer_template,recursive=True)\n",
    "existing_pred_df_buffer_names=[os.path.basename(path) for path in glob.glob(pred_df_buffer_template.replace(\"pred_df_buffer-[TRAIN_TEXT_LIMIT]-[TEST_TEXT_LIMIT].csv\",\"*.csv\"),recursive=True)]\n",
    "ppl_dfs_buffer_paths=[path for path in ppl_dfs_buffer_paths if os.path.basename(path).replace(\"ppl_dfs_buffer-\",\"pred_df_buffer-\") not in existing_pred_df_buffer_names]\n",
    "\n",
    "ppl_dfs_buffer_paths_batches=[list(i) for i in list((np.array_split(ppl_dfs_buffer_paths,proc_num)))]\n",
    "\n",
    "with Pool(proc_num) as p:\n",
    "    p.map(ppl_dfs_2_pred_df,ppl_dfs_buffer_paths_batches)\n",
    "\n",
    "pred_df_buffer_template=os.path.join(pred_df_buffer_home,\"*.csv\")\n",
    "benchmark_results_df_buffer_template=os.path.join(benchmark_results_df_home,\"benchmark_results_df_buffer-[TRAIN_TEXT_LIMIT]-[TEST_TEXT_LIMIT].csv\")\n",
    "\n",
    "def pred_df_2_benchmark_results_df(pred_df_buffer_paths):\n",
    "\n",
    "    for pred_df_buffer_path in tqdm(pred_df_buffer_paths):\n",
    "\n",
    "        train_text_limit=os.path.basename(pred_df_buffer_path).split(\".csv\")[0].split(\"-\")[1]\n",
    "        test_text_limit=os.path.basename(pred_df_buffer_path).split(\".csv\")[0].split(\"-\")[2]\n",
    "        benchmark_results_df_buffer_path=benchmark_results_df_buffer_template.replace(\"[TEST_TEXT_LIMIT]\",test_text_limit).replace(\"[TRAIN_TEXT_LIMIT]\",train_text_limit)\n",
    "\n",
    "        pred_df=pd.read_csv(pred_df_buffer_path)\n",
    "        pred_df=pred_df.drop(columns=[\"Unnamed: 0\"]).set_index(\"by\")\n",
    "\n",
    "        benchmark_results_df=pd.DataFrame()\n",
    "\n",
    "        features=list(set(list(pred_df.index)))\n",
    "        true_tags=list(set(pred_df[\"true_tag\"]))\n",
    "\n",
    "        for feature in features:\n",
    "\n",
    "            fscore=f1_score(pred_df[\"true_tag\"].loc[feature],pred_df[\"pred_tag\"].loc[feature],average=\"macro\")\n",
    "            precision=precision_score(pred_df[\"true_tag\"].loc[feature],pred_df[\"pred_tag\"].loc[feature],average=\"macro\")\n",
    "            recall=recall_score(pred_df[\"true_tag\"].loc[feature],pred_df[\"pred_tag\"].loc[feature],average=\"macro\")\n",
    "            accuracy=accuracy_score(pred_df[\"true_tag\"].loc[feature],pred_df[\"pred_tag\"].loc[feature])\n",
    "\n",
    "            benchmark_result={\"feature\":feature,\"true_tag\":\"GLOBAL\",\"fscore\":fscore,\"precision\":precision,\"recall\":recall,\"accuracy\":accuracy}\n",
    "            benchmark_results_df=pd.concat([benchmark_results_df,pd.DataFrame([benchmark_result])])\n",
    "\n",
    "            for true_tag in true_tags:\n",
    "\n",
    "                selected_pred_df=pred_df[pred_df[\"true_tag\"]==true_tag]\n",
    "\n",
    "                fscore=f1_score(selected_pred_df[\"true_tag\"].loc[feature],selected_pred_df[\"pred_tag\"].loc[feature],average=\"macro\")\n",
    "                precision=precision_score(selected_pred_df[\"true_tag\"].loc[feature],selected_pred_df[\"pred_tag\"].loc[feature],average=\"macro\")\n",
    "                recall=recall_score(selected_pred_df[\"true_tag\"].loc[feature],selected_pred_df[\"pred_tag\"].loc[feature],average=\"macro\")\n",
    "                accuracy=accuracy_score(selected_pred_df[\"true_tag\"].loc[feature],selected_pred_df[\"pred_tag\"].loc[feature])\n",
    "\n",
    "                benchmark_result={\"feature\":feature,\"true_tag\":true_tag,\"fscore\":fscore,\"precision\":precision,\"recall\":recall,\"accuracy\":accuracy}\n",
    "                benchmark_results_df=pd.concat([benchmark_results_df,pd.DataFrame([benchmark_result])])\n",
    "\n",
    "        benchmark_results_df.to_csv(benchmark_results_df_buffer_path) \n",
    "\n",
    "pred_df_buffer_paths=glob.glob(pred_df_buffer_template,recursive=True)\n",
    "existing_benchmark_results_df_buffer_names=[os.path.basename(path) for path in glob.glob(benchmark_results_df_buffer_template.replace(\"benchmark_results_df_buffer-[TRAIN_TEXT_LIMIT]-[TEST_TEXT_LIMIT].csv\",\"*.csv\"),recursive=True)]\n",
    "pred_df_buffer_paths=[path for path in pred_df_buffer_paths if os.path.basename(path).replace(\"pred_df_buffer-\",\"benchmark_results_df_buffer-\") not in existing_benchmark_results_df_buffer_names]\n",
    "\n",
    "pred_df_buffer_paths_batches=[list(i) for i in list((np.array_split(pred_df_buffer_paths,proc_num)))]\n",
    "\n",
    "with Pool(proc_num) as p:\n",
    "    p.map(pred_df_2_benchmark_results_df,pred_df_buffer_paths_batches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphplot.iristyle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b9ed231b0e477ad3928db4839744b6b66959899464f1da5fdaed485365feafe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
